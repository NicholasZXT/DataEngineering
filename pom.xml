<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <name>DataEngineering</name>
    <groupId>pers.zxt.idea</groupId>
    <artifactId>DataEngineering</artifactId>
    <version>1.0</version>
    <packaging>jar</packaging>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <!--指定编译的java版本-->
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <scala.version>2.11.12</scala.version>
        <hadoop.version>2.8.5</hadoop.version>
        <spark.version>2.4.8</spark.version>
        <!--scala的版本建议只需要写到2位，因为配合spark版本时，只需要前两位-->
        <spark.scala.version>2.11</spark.scala.version>
        <hbase.version>2.4.15</hbase.version>
        <!--<kafka.version>2.6.0</kafka.version>-->
        <!-- 调试 Flink 1.17 版本时，需要使用下面的kafka版本 -->
        <kafka.version>3.2.3</kafka.version>
        <!-- Flink 建议使用 1.13.6 或者 1.15.0 之后的版本-->
        <flink.version>1.17.1</flink.version>
        <!-- flink java包后面的scala版本后缀，从 flink 1.15 版本开始不需要这个后缀，但是之前的版本需要-->
        <flink.scala.suffix></flink.scala.suffix>
        <elasticsearch.version>7.17.8</elasticsearch.version>
        <jackson.version>2.12.3</jackson.version>
        <encoding>UTF-8</encoding>
    </properties>
    
    <profiles>
        <profile>
            <id>local</id>
            <properties>
                <local.scope>runtime</local.scope>
                <hadoop.scope>compile</hadoop.scope>
                <spark.scope>compile</spark.scope>
                <kafka.scope>compile</kafka.scope>
                <elasticsearch.scope>compile</elasticsearch.scope>
                <hbase.scope>compile</hbase.scope>
                <flink.scope>compile</flink.scope>
            </properties>
        </profile>
    </profiles>

    <build>
        <plugins>
            <!-- maven打包Scala代码必须要有下面的插件-->
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>4.5.2</version>
                <configuration>
                    <scalaVersion>${scala.version}</scalaVersion>
                    <recompileMode>incremental</recompileMode>
                </configuration>
                <!--必须要配置下面的executions才能将scala代码打入jar包-->
                <executions>
                    <execution>
                        <id>scala-compile-first</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>add-source</goal>
                            <goal>compile</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>scala-test-compile</id>
                        <phase>process-test-resources</phase>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>

    </build>

    <dependencies>
        <!-- 下面导入依赖的顺序也很重要，当出现依赖冲突时，会以先导入的为准-->
        <dependency>
            <!--  Junit测试框架 -->
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.13.2</version>
            <scope>test</scope>
        </dependency>
        <!-- ***************************************************** -->
        <!-- log4j 依赖 -->
        <!-- 下面这个是 log4j 1.x 版本的最新依赖，但是也在 2012 年就停止维护， 建议转向 2.x 版本的-->
        <!--<dependency>-->
        <!--    <groupId>log4j</groupId>-->
        <!--    <artifactId>log4j</artifactId>-->
        <!--    <version>1.2.17</version>-->
        <!--    <scope>compile</scope>-->
        <!--</dependency>-->
        <!-- log4j 2.x 版本的依赖被拆分成了下面的两个依赖-->
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-core</artifactId>
            <version>2.20.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-api</artifactId>
            <version>2.20.0</version>
        </dependency>
        <!-- ***************************************************** -->
        <!-- Logback + Slf4j 依赖 -->
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <version>1.2.13</version>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <version>1.2.13</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>1.7.36</version>
        </dependency>
        <!-- ***************************************************** -->
        <!--  spark-core依赖 -->
        <!--<dependency>-->
        <!--    <groupId>org.apache.spark</groupId>-->
        <!--    &lt;!&ndash;spark依赖的artifactId的后缀就是对应的scala版本&ndash;&gt;-->
        <!--    <artifactId>spark-core_${spark.scala.version}</artifactId>-->
        <!--    <version>${spark.version}</version>-->
        <!--    <scope>${spark.scope}</scope>-->
        <!--</dependency>-->
        <!-- 上面的 spark-core 可以不用指定，因为下面的spark依赖会自动导入-->
        <!-- 有时候 可能会需要手动导入下面的 jackson-module 依赖，spark-core 会依赖于这个-->
        <!--<dependency>-->
        <!--    <groupId>com.fasterxml.jackson.module</groupId>-->
        <!--    <artifactId>jackson-module-scala_${spark.scala.version}</artifactId>-->
        <!--    <version>2.8.8</version>-->
        <!--</dependency>-->
        <!-- 本地执行spark，但是提交到远程的 YARN 集群时，需要下面的这个依赖-->
        <!--<dependency>-->
        <!--    <groupId>org.apache.spark</groupId>-->
        <!--    <artifactId>spark-yarn_${spark.scala.version}</artifactId>-->
        <!--    <version>${spark.version}</version>-->
        <!--    <scope>${spark.scope}</scope>-->
        <!--</dependency>-->
        <dependency>
            <!-- Spark-SQL 依赖-->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>
        <dependency>
            <!-- Spark-Streaming 依赖-->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>
        <dependency>
            <!-- Spark-Streaming 接入 kafka-->
            <!-- 还有一个 Spark Project External Kafka 包提供的 spark-streaming-kafka_2.11，但是这个从2016年开始就不更新了，
            好像是旧版的kafka接口-->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>
        <dependency>
            <!--  Spark-ML 机器学习  -->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>
        <!-- ***************************************************** -->
        <!-- kafka 依赖 -->
        <dependency>
            <!-- 包含了 topic管理，Producer 和 Consumer 的API -->
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>${kafka.version}</version>
            <scope>${kafka.scope}</scope>
        </dependency>
        <dependency>
            <!-- 流式处理的 API -->
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-streams</artifactId>
            <version>${kafka.version}</version>
            <scope>${kafka.scope}</scope>
        </dependency>
        <!-- ***************************************************** -->
        <!--HBase依赖 -->
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>${hbase.version}</version>
            <scope>${hbase.scope}</scope>
        </dependency>
        <!--<dependency>-->
        <!--    <groupId>org.apache.hbase</groupId>-->
        <!--    <artifactId>hbase-server</artifactId>-->
        <!--    <version>${hbase.version}</version>-->
        <!--    <scope>${hbase.scope}</scope>-->
        <!--</dependency>-->
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-mapreduce</artifactId>
            <version>${hbase.version}</version>
            <scope>${hbase.scope}</scope>
        </dependency>
        <!-- ***************************************************** -->
        <!-- ElasticSearch依赖-->
        <dependency>
            <groupId>co.elastic.clients</groupId>
            <artifactId>elasticsearch-java</artifactId>
            <version>${elasticsearch.version}</version>
            <scope>${elasticsearch.scope}</scope>
        </dependency>
        <!--<dependency>-->
        <!--    <groupId>org.elasticsearch.client</groupId>-->
        <!--    <artifactId>elasticsearch-rest-high-level-client</artifactId>-->
        <!--    <version>${elasticsearch.version}</version>-->
        <!--    <scope>${elasticsearch.scope}</scope>-->
        <!--</dependency>-->
        <!--<dependency>-->
        <!--    <groupId>com.fasterxml.jackson.core</groupId>-->
        <!--    <artifactId>jackson-databind</artifactId>-->
        <!--    <version>${jackson.version}</version>-->
        <!--</dependency>-->
        <!-- ***************************************************** -->
        <!-- Flink 相关依赖 -->
        <dependency>
            <!-- flink的Java api, scala api是另外一个依赖-->
            <groupId>org.apache.flink</groupId>
            <!-- 从1.15版本之后，flink-java依赖不需要后面的scala版本后缀，在此之前的版本需要，其他的flink依赖类似-->
            <artifactId>flink-streaming-java${flink.scala.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <dependency>
            <!-- 本地从 main 方法运行作业时，需要这个依赖，用于提交作业-->
            <!-- 从 1.10 版本开始，使用上面的 flink-stream-java 时，必须要引入这个客户端了 -->
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients${flink.scala.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <dependency>
            <!-- 从文件读取数据需要这个依赖，从1.12版本开始才有 -->
            <groupId>org.apache.flink</groupId>
            <!-- 一直不需要scala版本后缀-->
            <artifactId>flink-connector-files</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <dependency>
            <!-- 数据生成器的依赖-->
            <groupId>org.apache.flink</groupId>
            <!-- 一直不需要scala版本后缀-->
            <artifactId>flink-connector-datagen</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <dependency>
            <!-- 连接kafka依赖 -->
            <!-- 这里依赖的kafka版本和上面我自己设置的 kafka 版本有冲突-->
            <groupId>org.apache.flink</groupId>
            <!-- 需要scala版本后缀-->
            <artifactId>flink-connector-kafka${flink.scala.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <!--
        Part I: 参见官方文档（1.13版本才有此说明） https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/dev/table/overview/.
        关于 Flink Table API/SQL，有几点需要注意：
        1. 从1.9开始，Flink 提供了两个 Table Planner 实现来执行 Table API 和 SQL 程序：Blink Planner 和 Old Planner，Old Planner 在1.9之前就已经存在了
        2. 对于生产环境，建议使用在 1.11 版本之后已经变成默认的Blink Planner

        Part II: 参见 1.14 版本更新日志 https://nightlies.apache.org/flink/flink-docs-master/release-notes/flink-1.14/#table-api-sql
        要注意的是：
        1. Table API/SQL 的 Old Planner 在这个版本正式被移除
        2. 有 3 个依赖包因为上述变动进行了改名：
           flink-table-planner-blink -> flink-table-planner
           flink-table-runtime-blink -> flink-table-runtime
           flink-table-uber-blink -> flink-table-uber
        -->
        <!-- flink-table/sql的依赖，只需要使用table时选这个，用的不多，一般直接用下面的 DataStream + Table/SQL -->
        <!--<dependency>-->
        <!--    <groupId>org.apache.flink</groupId>-->
        <!--    &lt;!&ndash; 不需要scala版本后缀 &ndash;&gt;-->
        <!--    <artifactId>flink-table-api-java</artifactId>-->
        <!--    <version>${flink.version}</version>-->
        <!--    <scope>${flink.scope}</scope>-->
        <!--</dependency>-->
        <dependency>
            <!-- flink-table/sql 和 DataStream 交互操作的依赖，它引入了上面的 flink-table-api-java-->
            <groupId>org.apache.flink</groupId>
            <!-- 需要scala版本后缀-->
            <artifactId>flink-table-api-java-bridge${flink.scala.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <!-- 下面是 两类 Flink Table/SQL 的查询计划引擎，1.11之后，优先选择 blink版本，1.14.0 之后，就只剩一个blink引擎了；
        但是 1.15 之后，引入方式又变成了使用一个封装的 查询计划加载器，见下面-->
        <!--<dependency>-->
        <!--    &lt;!&ndash; 1.13.6之前是 Old Planner 依赖，1.14.0开始，由下面的 flink-table-planner-blink 更名而来 &ndash;&gt;-->
        <!--    <groupId>org.apache.flink</groupId>-->
        <!--    &lt;!&ndash; 这个一直使用 2.12 的scala版本后缀，一般都没啥问题&ndash;&gt;-->
        <!--    <artifactId>flink-table-planner_2.12</artifactId>-->
        <!--    <version>${flink.version}</version>-->
        <!--    <scope>test</scope>-->
        <!--</dependency>-->
        <!--<dependency>-->
        <!--    &lt;!&ndash; blink Planner 引擎，但是只到 1.13.6，1.14.0 之后改成了上面的包名称 &ndash;&gt;-->
        <!--    <groupId>org.apache.flink</groupId>-->
        <!--    <artifactId>flink-table-planner-blink${flink.scala.suffix}</artifactId>-->
        <!--    <version>${flink.version}</version>-->
        <!--    <scope>${flink.scope}</scope>-->
        <!--</dependency>-->
        <!--
        Part III: 参考官方文档： https://nightlies.apache.org/flink/flink-docs-release-1.18/zh/docs/dev/configuration/advanced/
        1. 从 Flink 1.15.0 版本开始，又对查询计划引擎搞了一个下面的 查询计划加载器，用于解决scala版本问题。
        2. 它和上面 flink-table-planner 功能一样，但是打包方式不同，下面的这个加载器会自己处理scala版本问题 —— 两者不能同时存在！！！
        3. 从 1.15 之后，推荐使用下面的这个查询计划加载器，并且在未来的版本中，会考虑移除上面的 flink-table-planner
        -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-loader</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>

        <!-- table/sql 公共模块，比如自定义函数、自定义格式等扩展功能需要的最小依赖，这个会被很多flink依赖自动引入，通常不需要手动引入-->
        <!--<dependency>-->
        <!--    <groupId>org.apache.flink</groupId>-->
        <!--    <artifactId>flink-table-common</artifactId>-->
        <!--    <version>${flink.version}</version>-->
        <!--    <scope>${flink.scope}</scope>-->
        <!--</dependency>-->
        <dependency>
            <!-- table/sql 在TaskManager上运行需要的环境依赖，一般来说本地调试需要，集群上会提供 -->
            <!-- 需要注意的是 flink-table-runtime 的版本是从 1.14.0 开始的，它是由下面的 flink-table-runtime-blink 在 1.14 版本改名而来 -->
            <groupId>org.apache.flink</groupId>
            <!-- 需要scala后缀-->
            <artifactId>flink-table-runtime${flink.scala.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        <!-- 下面这个也是 table/sql 在TaskManager上运行需要的环境依赖，blink引擎，但是版本只到 1.13.6，在这之后改名为上面的 flink-table-runtime -->
        <!--<dependency>-->
        <!--    <groupId>org.apache.flink</groupId>-->
        <!--    <artifactId>flink-table-runtime-blink_2.12</artifactId>-->
        <!--    <version>1.13.6</version>-->
        <!--    <scope>provided</scope>-->
        <!--</dependency>-->
        <dependency>
            <!--和Hadoop配合时，可能需要这个兼容性依赖的帮助-->
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-hadoop-compatibility_2.12</artifactId>
            <version>${flink.version}</version>
            <scope>compile</scope>
        </dependency>
        <!-- ***************************************************** -->
        <!-- Hadoop相关依赖，强烈建议放在所有大数据组件依赖的后面，因为手动指定的hadoop版本很可能会和前面某个组件冲突 -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
            <scope>${hadoop.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
            <scope>${hadoop.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>${hadoop.version}</version>
            <scope>${hadoop.scope}</scope>
            <!-- 这里依赖的 netty 和 spark-sql 的冲突，可以使用 mvn -P "local" dependency:tree > dep.txt 查看 -->
            <exclusions>
                <exclusion>
                    <groupId>io.netty</groupId>
                    <artifactId>netty-all</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <!-- ***************************************************** -->
        <dependency>
            <!-- alibaba fastJSON -->
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.68</version>
            <scope>compile</scope>
        </dependency>
        <!-- ***************************************************** -->
        <dependency>
            <!-- Thrift依赖：https://mvnrepository.com/artifact/org.apache.thrift/libthrift -->
            <groupId>org.apache.thrift</groupId>
            <artifactId>libthrift</artifactId>
            <version>0.17.0</version>
        </dependency>
        <!-- ***************************************************** -->
        <!--<dependency>-->
        <!--    &lt;!&ndash; XGBoost &ndash;&gt;-->
        <!--    <groupId>ml.dmlc</groupId>-->
        <!--    <artifactId>xgboost4j_2.11</artifactId>-->
        <!--    <version>1.0.0</version>-->
        <!--    &lt;!&ndash;<scope>${local.scope}}</scope>&ndash;&gt;-->
        <!--</dependency>-->
        <!--<dependency>-->
        <!--    &lt;!&ndash; XGBoost for spark&ndash;&gt;-->
        <!--    <groupId>ml.dmlc</groupId>-->
        <!--    <artifactId>xgboost4j-spark_2.11</artifactId>-->
        <!--    <version>1.0.0</version>-->
        <!--    &lt;!&ndash;<scope>${local.scope}}</scope>&ndash;&gt;-->
        <!--</dependency>-->
    </dependencies>

</project>