<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <name>DataEngineering</name>
    <groupId>pers.zxt.idea</groupId>
    <artifactId>DataEngineering</artifactId>
    <version>1.0</version>
    <packaging>jar</packaging>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <!--指定编译的java版本-->
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <scala.version>2.11.12</scala.version>
        <hadoop.version>2.8.5</hadoop.version>
        <spark.version>2.4.8</spark.version>
        <!--scala的版本建议只需要写到2位，因为配合spark版本时，只需要前两位-->
        <spark.scala.version>2.11</spark.scala.version>
        <kafka.version>2.6.0</kafka.version>
        <hbase.version>2.4.15</hbase.version>
        <elasticsearch.version>7.17.8</elasticsearch.version>
        <jackson.version>2.12.3</jackson.version>
        <encoding>UTF-8</encoding>
    </properties>
    
    <profiles>
        <profile>
            <id>local</id>
            <properties>
                <local.scope>runtime</local.scope>
                <hadoop.scope>compile</hadoop.scope>
                <spark.scope>compile</spark.scope>
                <kafka.scope>compile</kafka.scope>
                <hbase.scope>compile</hbase.scope>
            </properties>
        </profile>
    </profiles>

    <build>
        <plugins>
            <!-- maven打包Scala代码必须要有下面的插件-->
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>4.5.2</version>
                <configuration>
                    <scalaVersion>${scala.version}</scalaVersion>
                    <recompileMode>incremental</recompileMode>
                </configuration>
                <!--必须要配置下面的executions才能将scala代码打入jar包-->
                <executions>
                    <execution>
                        <id>scala-compile-first</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>add-source</goal>
                            <goal>compile</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>scala-test-compile</id>
                        <phase>process-test-resources</phase>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>

    </build>

    <dependencies>
        <!-- 下面导入依赖的顺序也很重要，当出现依赖冲突时，会以先导入的为准-->
        <!--  Junit测试框架 -->
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.13.2</version>
            <scope>test</scope>
        </dependency>

        <!--  spark-core依赖 -->
        <!--<dependency>-->
        <!--    <groupId>org.apache.spark</groupId>-->
        <!--    &lt;!&ndash;spark依赖的artifactId的后缀就是对应的scala版本&ndash;&gt;-->
        <!--    <artifactId>spark-core_${spark.scala.version}</artifactId>-->
        <!--    <version>${spark.version}</version>-->
        <!--    <scope>${spark.scope}</scope>-->
        <!--</dependency>-->
        <!-- 上面的 spark-core 可以不用指定，因为下面的spark依赖会自动导入-->
        <!-- 有时候 可能会需要手动导入下面的 jackson-module 依赖，spark-core 会依赖于这个-->
        <!--<dependency>-->
        <!--    <groupId>com.fasterxml.jackson.module</groupId>-->
        <!--    <artifactId>jackson-module-scala_${spark.scala.version}</artifactId>-->
        <!--    <version>2.8.8</version>-->
        <!--</dependency>-->
        <!-- 本地执行spark，但是提交到远程的 YARN 集群时，需要下面的这个依赖-->
        <!--<dependency>-->
        <!--    <groupId>org.apache.spark</groupId>-->
        <!--    <artifactId>spark-yarn_${spark.scala.version}</artifactId>-->
        <!--    <version>${spark.version}</version>-->
        <!--    <scope>${spark.scope}</scope>-->
        <!--</dependency>-->
        <dependency>
            <!-- Spark-SQL 依赖-->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>
        <dependency>
            <!-- Spark-Streaming 依赖-->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>
        <dependency>
            <!-- Spark-Streaming 接入 kafka-->
            <!-- 还有一个 Spark Project External Kafka 包提供的 spark-streaming-kafka_2.11，但是这个从2016年开始就不更新了，
            好像是旧版的kafka接口-->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>
        <dependency>
            <!--  Spark-ML 机器学习  -->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>

        <!-- kafka 依赖 -->
        <dependency>
            <!-- 包含了 topic管理，Producer 和 Consumer 的API -->
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>${kafka.version}</version>
            <scope>${kafka.scope}</scope>
        </dependency>
        <dependency>
            <!-- 流式处理的 API -->
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-streams</artifactId>
            <version>${kafka.version}</version>
            <scope>${kafka.scope}</scope>
        </dependency>

        <!-- Hadoop依赖 ，必须放在最后，否则前面的 spark 没法运行-->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
            <scope>${hadoop.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
            <scope>${hadoop.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>${hadoop.version}</version>
            <scope>${hadoop.scope}</scope>
        </dependency>

         <!--HBase依赖 -->
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>${hbase.version}</version>
            <scope>${hbase.scope}</scope>
        </dependency>
        <!--<dependency>-->
        <!--    <groupId>org.apache.hbase</groupId>-->
        <!--    <artifactId>hbase-server</artifactId>-->
        <!--    <version>${hbase.version}</version>-->
        <!--    <scope>${hbase.scope}</scope>-->
        <!--</dependency>-->

        <!-- ElasticSearch依赖-->
        <dependency>
            <groupId>co.elastic.clients</groupId>
            <artifactId>elasticsearch-java</artifactId>
            <version>${elasticsearch.version}</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
            <version>${jackson.version}</version>
        </dependency>
        <!--<dependency>-->
        <!--    <groupId>org.elasticsearch.client</groupId>-->
        <!--    <artifactId>elasticsearch-rest-high-level-client</artifactId>-->
        <!--    <version>${elasticsearch.version}</version>-->
        <!--</dependency>-->

        <!--   alibaba fastJSON -->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.68</version>
            <scope>compile</scope>
        </dependency>

        <!-- Thrift依赖-->
        <!-- https://mvnrepository.com/artifact/org.apache.thrift/libthrift -->
        <dependency>
            <groupId>org.apache.thrift</groupId>
            <artifactId>libthrift</artifactId>
            <version>0.17.0</version>
        </dependency>

        <!--  XGBoost  -->
        <!--<dependency>-->
        <!--    <groupId>ml.dmlc</groupId>-->
        <!--    <artifactId>xgboost4j_2.11</artifactId>-->
        <!--    <version>1.0.0</version>-->
        <!--    &lt;!&ndash;<scope>${local.scope}}</scope>&ndash;&gt;-->
        <!--</dependency>-->
        <!--<dependency>-->
        <!--    <groupId>ml.dmlc</groupId>-->
        <!--    <artifactId>xgboost4j-spark_2.11</artifactId>-->
        <!--    <version>1.0.0</version>-->
        <!--    &lt;!&ndash;<scope>${local.scope}}</scope>&ndash;&gt;-->
        <!--</dependency>-->

    </dependencies>

</project>