<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <name>DataEngineering</name>
    <groupId>pers.zxt.idea</groupId>
    <artifactId>DataEngineering</artifactId>
    <version>1.0</version>
    <packaging>jar</packaging>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <encoding>UTF-8</encoding>
        <!--指定编译的java版本-->
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <!-- 指定编译的scala版本，这个设置会被 scala-maven-plugin 使用，不太推荐手动设置 scala 版本，原因参见插件的说明-->
        <!--<scala.version>2.11.12</scala.version>-->
        <scala.version>2.12.18</scala.version>
        <!--<hadoop.version>2.8.5</hadoop.version>-->
        <hadoop.version>3.1.1</hadoop.version>
        <mapreduce.version>3.1.1</mapreduce.version>
        <spark.version>2.4.8</spark.version>
        <!--scala的版本建议只需要写到2位，因为配合spark版本时，只需要前两位-->
        <spark.scala.version>2.12</spark.scala.version>
        <elasticsearch.version>7.17.8</elasticsearch.version>
        <!--<kafka.version>2.6.0</kafka.version>-->
        <!-- 调试 Flink 1.17 版本时，需要使用下面的kafka版本 -->
        <kafka.version>3.2.3</kafka.version>
        <!-- Flink 建议使用 1.13.6 或者 1.15.0 之后的版本；flink java api在1.15版本之前需要一个scala版本后缀，1.15开始去掉了-->
        <!--<flink.version>1.13.6</flink.version>-->
        <!--<flink.java.suffix>_2.12</flink.java.suffix>-->
        <flink.version>1.17.1</flink.version>
        <flink.java.suffix></flink.java.suffix>
        <!-- flink scala api的scala版本后缀，一直需要-->
        <flink.scala.suffix>_2.12</flink.scala.suffix>
        <hbase.version>2.4.15</hbase.version>
        <!-- 注意和 Spark 的版本冲突-->
        <!--<jackson.version>2.12.3</jackson.version>-->
        <jackson.version>2.6.7</jackson.version>
    </properties>
    
    <profiles>
        <profile>
            <id>local</id>
            <properties>
                <local.scope>compile</local.scope>
                <hadoop.scope>compile</hadoop.scope>
                <mapreduce.scope>compile</mapreduce.scope>
                <spark.scope>compile</spark.scope>
                <kafka.scope>compile</kafka.scope>
                <elasticsearch.scope>compile</elasticsearch.scope>
                <hbase.scope>compile</hbase.scope>
                <flink.scope>compile</flink.scope>
            </properties>
        </profile>
        <profile>
            <id>deploy</id>
            <properties>
                <local.scope>provided</local.scope>
                <hadoop.scope>provided</hadoop.scope>
                <mapreduce.scope>compile</mapreduce.scope>
                <spark.scope>provided</spark.scope>
                <kafka.scope>provided</kafka.scope>
                <elasticsearch.scope>provided</elasticsearch.scope>
                <hbase.scope>provided</hbase.scope>
                <flink.scope>provided</flink.scope>
            </properties>
        </profile>
    </profiles>

    <build>
        <plugins>
            <!-- maven打包Scala代码必须要有下面的插件-->
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>4.5.2</version>
                <configuration>
                    <!-- 不推荐在这里设置 scala 的版本，默认下插件会从依赖中自己推断，也不推荐在 properties 中通过 scala.version 属性设置 -->
                    <!-- 因为此插件会从Maven依赖中寻找设置的scala版本，如果引入的依赖里（比如spark-streaming的传递依赖里）没找到，
                    那么刷新Maven时IDEA会报错（但不一定影响编译过程）；
                    如果非要手动设置scala版本，要么设置成Maven依赖里能找到的scala版本，要么在 dependencies 里手动引入 scala-library
                    -->
                    <!--<scalaVersion>${scala.version}</scalaVersion>-->
                    <recompileMode>incremental</recompileMode>
                </configuration>
                <!--必须要配置下面的executions才能将scala代码打入jar包-->
                <executions>
                    <execution>
                        <id>scala-compile-first</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>add-source</goal>
                            <goal>compile</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>scala-test-compile</id>
                        <phase>process-test-resources</phase>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <!-- 大数据打包需要用到这个插件 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.0.0</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <artifactSet>
                                <excludes>
                                    <!-- 排除掉日志相关的依赖-->
                                    <exclude>com.google.code.findbugs:jsr305</exclude>
                                    <exclude>org.slf4j:*</exclude>
                                    <exclude>log4j:*</exclude>
                                    <exclude>ch.qos.logback:*</exclude>
                                </excludes>
                            </artifactSet>
                            <filters>
                                <filter>
                                    <!-- 排除掉一些敏感资源和日志配置文件 -->
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                        <exclude>log4j2.properties</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <!-- 设置jar包的主入口类 -->
                                    <mainClass>HelloJava</mainClass>
                                </transformer>
                                <!-- Flink SQL Connector部署打包时需要这个资源转换器-->
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>

    </build>

    <dependencies>
        <!-- 下面导入依赖的顺序也很重要，当出现依赖冲突时，会以先导入的为准-->
        <dependency>
            <!--  Junit测试框架 -->
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.13.2</version>
            <scope>test</scope>
        </dependency>
        <!-- ***************************************************** -->
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>1.16.22</version>
        </dependency>
        <!-- Alibaba fastJSON -->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.68</version>
            <scope>compile</scope>
        </dependency>
        <!-- Jackson 工具，下面的依赖会自动引入 jackson-core 和 jackson-annotations -->
        <!-- 这里手动引入时，版本需要特别注意，可能会和 Spark 引入的Jackson有冲突-->
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
            <version>${jackson.version}</version>
            <scope>compile</scope>
        </dependency>

        <!-- ***************************************************** -->
        <!-- Apache Common-Cli 命令行解析工具-->
        <!-- https://mvnrepository.com/artifact/commons-cli/commons-cli -->
        <dependency>
            <groupId>commons-cli</groupId>
            <artifactId>commons-cli</artifactId>
            <version>1.5.0</version>
        </dependency>

        <!-- ***************************************************** -->
        <!-- 手动加入 Scala 的依赖，仅在强制设置 scala 版本时需要手动引入-->
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
            <scope>${local.scope}</scope>
        </dependency>

        <!-- ***************************************************** -->
        <!--  spark-core依赖 -->
        <!--<dependency>-->
        <!--    <groupId>org.apache.spark</groupId>-->
        <!--    &lt;!&ndash;spark依赖的artifactId的后缀就是对应的scala版本&ndash;&gt;-->
        <!--    <artifactId>spark-core_${spark.scala.version}</artifactId>-->
        <!--    <version>${spark.version}</version>-->
        <!--    <scope>${spark.scope}</scope>-->
        <!--</dependency>-->
        <!-- 上面的 spark-core 可以不用指定，因为下面的spark依赖会自动导入-->
        <!-- 有时候 可能会需要手动导入下面的 jackson-module 依赖，spark-core 会依赖于这个-->
        <!--<dependency>-->
        <!--    <groupId>com.fasterxml.jackson.module</groupId>-->
        <!--    <artifactId>jackson-module-scala_${spark.scala.version}</artifactId>-->
        <!--    <version>2.8.8</version>-->
        <!--</dependency>-->
        <!-- 本地执行spark，但是提交到远程的 YARN 集群时，需要下面的这个依赖-->
        <!--<dependency>-->
        <!--    <groupId>org.apache.spark</groupId>-->
        <!--    <artifactId>spark-yarn_${spark.scala.version}</artifactId>-->
        <!--    <version>${spark.version}</version>-->
        <!--    <scope>${spark.scope}</scope>-->
        <!--</dependency>-->
        <dependency>
            <!-- Spark-SQL 依赖-->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>
        <dependency>
            <!-- Spark-Streaming 依赖-->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
            <!--<exclusions>-->
            <!--    <exclusion>-->
            <!--        <groupId>org.scala-lang</groupId>-->
            <!--        <artifactId>scala-library</artifactId>-->
            <!--    </exclusion>-->
            <!--</exclusions>-->
        </dependency>
        <dependency>
            <!-- Spark-Streaming 接入 kafka-->
            <!-- 还有一个 Spark Project External Kafka 包提供的 spark-streaming-kafka_2.11，但是这个从2016年开始就不更新了，
            好像是旧版的kafka接口-->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>
        <dependency>
            <!--  Spark-ML 机器学习  -->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>${spark.scope}</scope>
        </dependency>

        <!-- ***************************************************** -->
        <!-- kafka 依赖 -->
        <dependency>
            <!-- 包含了 topic管理，Producer 和 Consumer 的API -->
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>${kafka.version}</version>
            <scope>${kafka.scope}</scope>
        </dependency>
        <dependency>
            <!-- 流式处理的 API -->
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-streams</artifactId>
            <version>${kafka.version}</version>
            <scope>${kafka.scope}</scope>
        </dependency>

        <!-- ***************************************************** -->
        <!--HBase依赖 -->
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>${hbase.version}</version>
            <scope>${hbase.scope}</scope>
        </dependency>
        <!--<dependency>-->
        <!--    <groupId>org.apache.hbase</groupId>-->
        <!--    <artifactId>hbase-server</artifactId>-->
        <!--    <version>${hbase.version}</version>-->
        <!--    <scope>${hbase.scope}</scope>-->
        <!--</dependency>-->
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-mapreduce</artifactId>
            <version>${hbase.version}</version>
            <scope>${hbase.scope}</scope>
        </dependency>

        <!-- ***************************************************** -->
        <!-- ElasticSearch依赖-->
        <dependency>
            <groupId>co.elastic.clients</groupId>
            <artifactId>elasticsearch-java</artifactId>
            <version>${elasticsearch.version}</version>
            <scope>${elasticsearch.scope}</scope>
        </dependency>
        <!--<dependency>-->
        <!--    <groupId>org.elasticsearch.client</groupId>-->
        <!--    <artifactId>elasticsearch-rest-high-level-client</artifactId>-->
        <!--    <version>${elasticsearch.version}</version>-->
        <!--    <scope>${elasticsearch.scope}</scope>-->
        <!--</dependency>-->

        <!-- ***************************************************** -->
        <!-- Flink 相关依赖 -->
        <!-- Flink DataStream依赖-->
        <dependency>
            <!-- Flink DataStream 的 Java API-->
            <groupId>org.apache.flink</groupId>
            <!-- 从1.15版本之后，flink-java依赖不需要后面的scala版本后缀，在此之前的版本需要，其他的flink依赖类似-->
            <artifactId>flink-streaming-java${flink.java.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <dependency>
            <!-- Flink DataStream 的 Scala API, scala相关的所有依赖，都需要版本后缀 -->
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala${flink.scala.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
            <!--<exclusions>-->
            <!--    <exclusion>-->
            <!--        <groupId>org.scala-lang</groupId>-->
            <!--        <artifactId>scala-library</artifactId>-->
            <!--    </exclusion>-->
            <!--    <exclusion>-->
            <!--        <groupId>org.scala-lang</groupId>-->
            <!--        <artifactId>scala-reflect</artifactId>-->
            <!--    </exclusion>-->
            <!--    <exclusion>-->
            <!--        <groupId>org.scala-lang</groupId>-->
            <!--        <artifactId>scala-compiler</artifactId>-->
            <!--    </exclusion>-->
            <!--</exclusions>-->
        </dependency>
        <dependency>
            <!-- 本地从 main 方法运行作业时，需要这个依赖，用于提交作业-->
            <!-- 从 1.10 版本开始，使用上面的 flink-stream-java 时，必须要引入这个客户端了 -->
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients${flink.java.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <!-- Flink Connector 相关依赖-->
        <dependency>
            <!-- 从文件读取数据需要这个依赖，从1.12版本开始才有 -->
            <groupId>org.apache.flink</groupId>
            <!-- 一直不需要scala版本后缀-->
            <artifactId>flink-connector-files</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <dependency>
            <!-- 数据生成器的依赖, 1.17 版本才开始有-->
            <groupId>org.apache.flink</groupId>
            <!-- 一直不需要scala版本后缀-->
            <artifactId>flink-connector-datagen</artifactId>
            <version>${flink.version}</version>
            <scope>compile</scope>
        </dependency>
        <!-- hive连接器依赖 -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-hive${flink.scala.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>compile</scope>
        </dependency>
        <!-- kafka连接器依赖，用于配合 DataStream API 使用 -->
        <!-- 这里依赖的kafka版本和上面我自己设置的 kafka 版本有冲突-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <!-- 1.15版本之前需要scala版本后缀，之后不需要-->
            <artifactId>flink-connector-kafka${flink.java.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>compile</scope>
        </dependency>
        <!-- kafka连接器 SQL依赖，这个不会引入上面的kafka依赖 -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-sql-connector-kafka${flink.java.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>compile</scope>
        </dependency>
        <!-- Flink Table API/SQL 依赖-->
        <!-- Flink-table/sql的java依赖，只需要使用table时选这个，用的不多，一般直接用下面的 Table/SQL + DataStream -->
        <!--<dependency>-->
        <!--    <groupId>org.apache.flink</groupId>-->
        <!--    <artifactId>flink-table-api-java</artifactId>-->
        <!--    <version>${flink.version}</version>-->
        <!--    <scope>${flink.scope}</scope>-->
        <!--</dependency>-->
        <!-- Flink-table/sql + DataStream 交互操作的java依赖，它会引入上面的 flink-table-api-java -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <!-- 1.15之前需要scala版本后缀，1.15之后不需要-->
            <artifactId>flink-table-api-java-bridge${flink.java.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <!-- Flink-Table API/SQL 的 Scala 依赖，始终需要指定 scala 版本后缀，一般会直接用下面的 -->
        <!--<dependency>-->
        <!--    <groupId>org.apache.flink</groupId>-->
        <!--    <artifactId>flink-table-api-scala${flink.scala.suffix}</artifactId>-->
        <!--    <version>${flink.version}}</version>-->
        <!--    <scope>${flink.scope}</scope>-->
        <!--</dependency>-->
        <!-- Flink-Table API/SQL + DataStream 的 Scala 依赖，始终需要 scala 版本后缀-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-scala-bridge${flink.scala.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <!-- Flink-Table API/SQL 的查询计划引擎依赖 -->
        <!-- 1.15版本之后，直接使用这个即可；1.15之前，需要使用 flink-table-planner${flink.scala.suffix} -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-loader</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <!-- 其他依赖 -->
        <!-- table/sql 在TaskManager上运行需要的环境依赖，一般来说本地调试需要，集群上会提供 -->
        <!-- 需要注意的是 flink-table-runtime 的版本是从 1.14.0 开始的，它是由 flink-table-runtime-blink 在 1.14 版本改名而来 -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <!-- 需要scala后缀-->
            <artifactId>flink-table-runtime${flink.java.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>${flink.scope}</scope>
        </dependency>
        <dependency>
            <!--和Hadoop配合时，可能需要这个兼容性依赖的帮助，scope必须是compile-->
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-hadoop-compatibility${flink.scala.suffix}</artifactId>
            <version>${flink.version}</version>
            <scope>compile</scope>
        </dependency>
        <!-- Flink-CDC 依赖-->
        <!-- Flink-CDC 2.0.0 ~ 3.0.1 版本使用的是如下包名空间 -->
        <dependency>
            <groupId>com.ververica</groupId>
            <artifactId>flink-connector-mysql-cdc</artifactId>
            <version>2.4.2</version>
            <!-- 下面的 3.0.1 版本和 Flink-1.17 有依赖冲突 -->
            <!--<version>3.0.1</version>-->
        </dependency>
        <!-- Flink-CDC 从 3.1.x 版本使用如下包名空间 -->
        <!--<dependency>-->
        <!--    <groupId>org.apache.flink</groupId>-->
        <!--    <artifactId>flink-connector-mysql-cdc</artifactId>-->
        <!--    <version>3.1.0</version>-->
        <!--</dependency>-->

        <!-- ***************************************************** -->
        <!-- Hadoop相关依赖，强烈建议放在所有大数据组件依赖的后面，因为手动指定的hadoop版本很可能会和前面某个组件冲突 -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
            <scope>${hadoop.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
            <scope>${hadoop.scope}</scope>
        </dependency>
        <!--<dependency>-->
        <!--    <groupId>org.apache.hadoop</groupId>-->
        <!--    <artifactId>hadoop-hdfs</artifactId>-->
        <!--    <version>${hadoop.version}</version>-->
        <!--    <scope>${hadoop.scope}</scope>-->
        <!--    &lt;!&ndash; 这里依赖的 netty 和 spark-sql 的冲突，可以使用 mvn -P "local" dependency:tree > dep.txt 查看 &ndash;&gt;-->
        <!--    <exclusions>-->
        <!--        <exclusion>-->
        <!--            <groupId>io.netty</groupId>-->
        <!--            <artifactId>netty-all</artifactId>-->
        <!--        </exclusion>-->
        <!--    </exclusions>-->
        <!--</dependency>-->
        <!-- Flink 需要下面的hadoop组件，而且必须是compile-->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-core</artifactId>
            <version>${mapreduce.version}</version>
            <scope>${mapreduce.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-common</artifactId>
            <version>${mapreduce.version}</version>
            <scope>${mapreduce.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
            <version>${mapreduce.version}</version>
            <scope>${mapreduce.scope}</scope>
        </dependency>

        <!-- ***************************************************** -->
        <!-- 手动导入的日志依赖放末尾，避免和上面某个组件引入的日志依赖冲突，特别是涉及到 Slf4j 的多个绑定时-->
        <!-- log4j 依赖 -->
        <!-- 下面这个是 log4j 1.x 的最后一个版本，但是在 2012 年就停止维护， 建议转向 2.x 版本的-->
        <!--<dependency>-->
        <!--    <groupId>log4j</groupId>-->
        <!--    <artifactId>log4j</artifactId>-->
        <!--    <version>1.2.17</version>-->
        <!--    <scope>compile</scope>-->
        <!--</dependency>-->
        <!-- log4j 2.x 版本的依赖被拆分成了下面的两个依赖-->
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-core</artifactId>
            <version>2.20.0</version>
            <scope>${local.scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-api</artifactId>
            <version>2.20.0</version>
            <scope>${local.scope}</scope>
        </dependency>
        <!-- ***************************************************** -->
        <!-- Slf4j + Logback 依赖 -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>1.7.36</version>
            <scope>${local.scope}</scope>
        </dependency>
        <!-- 上面的 Spark 依赖里使用的是 Slf4j + Log4j-1.2.17 的组合，因此会覆盖这里的 Logback-->
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <version>1.2.13</version>
            <scope>${local.scope}</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <version>1.2.13</version>
            <scope>${local.scope}</scope>
        </dependency>

        <!-- ***************************************************** -->
        <!-- Thrift依赖：https://mvnrepository.com/artifact/org.apache.thrift/libthrift -->
        <dependency>
            <groupId>org.apache.thrift</groupId>
            <artifactId>libthrift</artifactId>
            <version>0.17.0</version>
        </dependency>

        <!-- ***************************************************** -->
        <!-- XGBoost 和 XGBoost for Spark 依赖 -->
        <!--<dependency>-->
        <!--    <groupId>ml.dmlc</groupId>-->
        <!--    <artifactId>xgboost4j_2.11</artifactId>-->
        <!--    <version>1.0.0</version>-->
        <!--</dependency>-->
        <!--<dependency>-->
        <!--    <groupId>ml.dmlc</groupId>-->
        <!--    <artifactId>xgboost4j-spark_2.11</artifactId>-->
        <!--    <version>1.0.0</version>-->
        <!--</dependency>-->
    </dependencies>

</project>